import nltk
from nltk.tokenize import sent_tokenize, word_tokenize
nltk.download('punkt')
nltk.download('stopwords')
from nltk.corpus import stopwords
import pandas as pd
import string
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import CountVectorizer

# Define method to normalize text by lower casing, removing punctuations, stopwords. Numbers are retained, and some essential punctuations  are 
# retained
stop_words = nltk.corpus.stopwords.words('english') 


def normalize_document(doc):
    # lower case and remove special characters\whitespaces
    #doc = re.sub(r'[^\a-zA-Z0-9\'\s]', '', doc, re.I) #[^a-zA-Z0-9\s]
    doc = doc.lower() 
    doc = doc.strip()
    # tokenize document
    # split into words by white space
    words = doc.split()
    table = str.maketrans('', '', '!"#$%&()*+,./:;<=>?@[\\]^_{|}~') #retaining punc like ' which are part of italian and french languages
    stripped = [w.translate(table) for w in words]
    # filter stopwords out of document
    filtered_tokens = [token for token in stripped if token not in stop_words]
    # re-create document from filtered tokens
    doc = ' '.join(filtered_tokens)
    return doc


data = pd.read_csv('/content/Input_RFP-Requirements.csv',encoding='ISO-8859-1')
data = data['Requirement']
data.shape

norm_doc1 = []
for i in range(len(data)):
    norm_data1 = normalize_document(data[i])
    norm_doc1.append(norm_data1)

#The New Requirement statement is taken from the user and its tokenized and stopwords are being removed.

#The New Requirement statement is taken from the user and its tokenized and stopwords are being removed.
datanew= pd.read_csv('/content/Input_RFP-Requirementsnew.csv',encoding='ISO-8859-1')
datanew = datanew['Requirement']
datanew.shape

norm_doc2 = []
for i in range(len(datanew)):
    norm_data2 = normalize_document(datanew[i])
    norm_doc2.append(norm_data2)



#cosine similarity
vectorizer = CountVectorizer().fit_transform(norm_doc1)
vectors1 = vectorizer.toarray()
vectors1
csim1=cosine_similarity(vectors1)
csim1


def cosine_sim_vectors(vec1, vec2):
  vec1 = vec1.reshape(1, -1)
  vec2 = vec2.reshape(1, -1)
  return cosine_similarity(vec1, vec2)[0][0]
cosine_sim_vectors(vectors1[0],vectors1[1])

#Jaccard's Similarity function
def compute_jaccard_similarity_score(x, y):
    intersection_cardinality = len(set(x).intersection(set(y)))
    union_cardinality = len(set(x).union(set(y)))
    return intersection_cardinality / float(union_cardinality)

#Correlation of the New Requirement with previous requirement is computed and is put in list.
corr=[]
for i in range(len(norm_doc1)):
  corr.append(compute_jaccard_similarity_score(norm_doc2[0],norm_doc1[i]))
corr
   
max_js=max(corr)

if max_js == 0 :
    print(f'There is no similar requirement which has been implemented.')
	#break
else:
    print(f'The MOST similar requirement(s) already implemented are:')
    for i in range(len(corr)):
        if corr[i] == max_js:
            print(f'"',norm_doc1[i],'"')
            print(f'The Jaccard similarity is ', corr[i],'\n')
